{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape ISBN from Google Books page based on provided HTML structure\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('books_data.csv')\n",
    "\n",
    "df['previewLink'] = df['previewLink'].fillna('')\n",
    "\n",
    "def shorten_google_books_link(full_link):\n",
    "    # Find the index of the '&' character to get the substring before it\n",
    "    index = full_link.find('&')\n",
    "    if index != -1:  # If '&' character is found\n",
    "        shortened_link = full_link[:index]\n",
    "    else:  # If '&' character is not found, return the original link\n",
    "        shortened_link = full_link\n",
    "    return shortened_link\n",
    "\n",
    "# Apply shorten_google_books_link function to create a new column with shortened links\n",
    "df['ShortenedLink'] = df['previewLink'].apply(shorten_google_books_link)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    # Apply the scraping function to the chunk and add ISBN column\n",
    "    chunk['ISBN'] = chunk['ShortenedLink'].apply(scrape_isbn)\n",
    "    return chunk\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(num_cores)\n",
    "\n",
    "chunk_size = 250\n",
    "start_index = 20000\n",
    "end_index = start_index + chunk_size\n",
    "output_file = \"processed_data_new2.csv\"\n",
    "while start_index < len(df):\n",
    "    chunk = df.iloc[start_index:end_index]\n",
    "    processed_chunk = pool.apply(process_chunk, args=(chunk,))\n",
    "    # Do something with processed_chunk, e.g., save it to a file\n",
    "    # Inside the loop where chunks are processed\n",
    "    with open(output_file, 'a') as f:\n",
    "      processed_chunk.to_csv(f, index=False, header=not start_index)  # Only write header for the first chunk\n",
    "    print(f\"Appended processed chunk with start index: {start_index} to {output_file}\")\n",
    "\n",
    "    # Update start_index and end_index for the next chunk\n",
    "    start_index = end_index\n",
    "    end_index = min(start_index + chunk_size, len(df))\n",
    "\n",
    "#processed_chunk = pool.apply(process_chunk, args=(chunk,))\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['books_with_isbn_chunk_1.csv', 'books_with_isbn_chunk_2.csv', 'books_with_isbn_chunk_3.csv', \n",
    "              'books_with_isbn_chunk_4.csv', 'books_with_isbn_chunk_5.csv', 'books_with_isbn_chunk_6.csv', \n",
    "              'books_with_isbn_chunk_7.csv', 'books_with_isbn_chunk_8.csv', 'books_with_isbn_chunk_9.csv', \n",
    "              'books_with_isbn_chunk_13.csv', 'books_with_isbn_chunk_12.csv', 'books_with_isbn_chunk_11.csv', 'processed_data.csv',\n",
    "              'processed_data2.csv', 'processed_data4.csv', 'processed_data5.csv', 'processed_data6.csv', 'processed_data7.csv',\n",
    "              'processed_data8.csv', 'processed_data10.csv', 'processed_data11.csv', 'processed_data12.csv', 'processed_data13.csv', 'processed_data15.csv',\n",
    "              'processed_data16.csv', 'processed_data17.csv']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each CSV file path\n",
    "for file_path in file_paths:\n",
    "    # Read the CSV file into a DataFrame and append it to the list\n",
    "    dfs.append(pd.read_csv(file_path))\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "concatenated_df = pd.concat(dfs, ignore_index=True, axis=1)\n",
    "\n",
    "# Write the concatenated DataFrame to a new CSV file\n",
    "concatenated_df.to_csv('datawithISBN.csv', index=False)\n",
    "\n",
    "\n",
    "newdf = pd.read_csv('datawithISBN.csv')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
